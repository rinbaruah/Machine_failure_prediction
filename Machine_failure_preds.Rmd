---
title: "Binary Machine Failure Prediction"
author: "Arindam Baruah"
date: "2023-06-13"
output:
  bookdown::html_document2:
   
    toc: false
    toc_float: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```






# Introduction

Binary machine failure prediction using machine learning is a technique employed to anticipate the occurrence of failures or malfunctions in a binary system or machine. With the increasing complexity of modern machines, the ability to predict and prevent failures becomes crucial for optimizing performance, reducing downtime, and avoiding costly repairs.

Machine learning algorithms play a vital role in this prediction process by analyzing historical data and identifying patterns or anomalies that indicate potential failures. These algorithms learn from past failure instances, considering various factors such as sensor readings, environmental conditions, maintenance records, and other relevant parameters.

The predictive models are trained on labeled datasets, where each instance is associated with a failure or non-failure outcome. Common machine learning techniques used for binary machine failure prediction include __logistic regression, decision trees, random forests, support vector machines (SVM), and neural networks__.

During the training phase, the algorithms learn the relationships between input features and failure occurrences, thereby enabling them to make accurate predictions on unseen data. Feature engineering, which involves selecting or transforming relevant input variables, is an essential step in improving the model's performance.

Once the model is trained, it can be deployed to make real-time predictions on new data streams. By continuously monitoring machine inputs and comparing them to the learned patterns, the system can generate alerts or take preventive actions whenever a potential failure is detected. This proactive approach helps minimize unexpected downtime, reduce maintenance costs, and improve overall operational efficiency.

Binary machine failure prediction using machine learning is widely applied across various industries, including manufacturing, power generation, healthcare, transportation, and more. By leveraging the power of data and advanced analytics, it offers a valuable tool for optimizing maintenance strategies, enhancing productivity, and ensuring the reliability of critical systems.

```{r fig.align='center',fig.height=3, fig.width=3, fig.cap="Source: www.gesrepair.com"}
knitr::include_graphics("mach_failure.jpeg")
```

# Importing the relevant libraries and dataset üõ†Ô∏è

First, we import the required libraries which we will use to perform the current analysis.

```{r}
library(tidyverse)
library(naniar)
library(bookdown)
library(stringr)
library(stringi)
library(lubridate)
library(DT)
library(forcats)
library(ggthemes)
library(corrplot)
library(mltools)
library(data.table)
library(visdat)
library(janitor)
library(cowplot)
```

Great ! We have all the libraries loaded. Next, we are gonna load the required dataset for conducting the machine failure classification analysis. 

We will use one dataset for the purpose of exploratory data analysis and training the classification model while the test dataset for testing the classification model on a completely new dataset.

After reading the data, let us see how the train dataset looks like.

```{r read-data}

df_train <- read_csv("data/train.csv")
df_test <-  read_csv("data/test.csv")
head(df_train)
```

We can observe that there are multiple process parameters present in the dataset which can help us analyse whether a machine undergoes failure. We can also observe that there are multiple abbreviations in this dataset. Let us try to understand what do these abbreviations mean :

1. __Tool Wear Failure (TWF)__: A type of machine failure which is associated with excessive tool wear.
2. __Heat Dissipation Failure (HDF)__: Machine failures which are associated with high process temperatures.
3. __Power Failure (PWF)__: Machine failures which are associated with power readings above or below a certain value.
4. __Overstrain Failure (OSF)__: Machine failures which are associated with high strain values.
5. __Random Failure (RNF)__:  Machine failures which maybe associated with random conditions.

# Data cleaning

## Check for null values

As a part of checking for the cleanliness of the dataset, let us visaulise the presence of null values for each of the variables.

```{r missvis,fig.cap="Missingness in the dataset",fig.align='center'}

gg_miss_var(df_train)
```
As we can observe from figure \@ref(fig:missvis), there are no missing values for any of the variables in the dataset. As a result, the __dataset can be considered clean__ for further analysis.

## Removal of variables

After studying for the presence of null values, we now remove the variables that do not provide any extra insights into our analysis.

```{r remove-var}

df_train <- df_train %>% select(-c(id,`Product ID`))

```

## Cleaning the variable names

The current dataset contains variable names which are not ideal for data wrangling and EDA. Hence, we will try to remove any unnecessary white space and special characters for each of the variable names.

```{r clean-names}

df_train <- clean_names(df_train)
head(df_train)

```



# Exploratory Data Analysis

After obtaining the cleansed dataset, we now try to visualise the relationship of each of the variables and attempt to obtain critical insights.

## Type of machine

There are a total of 3 types  machines in this dataset. These are encoded as:

1. __L (Light)__
2. __M (Medium)__
3. __H (Heavy)__

Let us see the number of machine failures for each of the machine types.

```{r failtype, fig.cap="Number of machine failures for each type", fig.align='center'}

df_type_group <- df_train %>% group_by(type,machine_failure) %>% summarise(count = n())
df_type_group <- df_type_group %>% group_by(type) %>% mutate(total = sum(count))
df_type_group <- df_type_group %>% mutate(percentage = 100 * (count/total))
pl1 <- ggplot(data = df_type_group, 
              aes(x = factor(machine_failure),
                  y = count)) + geom_col(aes(fill = type),color='black') + facet_wrap(~type) + geom_label(aes(label = count)) + labs(x = "Machine failure status", 
                                                                                                                       y = "Number of incidents") + ggtitle("Number of machine failures for each type") + theme_classic() + theme(legend.position = 'none') 
pl1
```


Figure \@ref(fig:failtype) illustrates the number of failures observed for each machine type. The failures constitute:\
- __1 %__ of the incidents for machine type "H"\
- __2 %__ of the incidents for machine type "L"\
- __1 %__ of the incidents for machine type "M"\

Hence, we can observe that the number of failure cases are __fairly evenly distributed among each of the machine types.__

## Air and process temperatures

Temperatures can play a critical role in relation to machine health. In this dataset, we have air and process temperatures. The difference of these values could allow us to understand the overall heat dissipation of the machines. Analysing these variables may allow us when do the machines undergo overall failure as well as heat dissipation failure (HDF).

Leet us first study the distribution of the temperature values.

```{r temps, fig.cap="Temperature variation distribution",fig.align='center'}

pl2 <- ggplot(data = df_train,
              aes(x = process_temperature_k,
                  fill = factor(hdf)),
              alpha = 0.6) + geom_histogram(alpha = 0.8,
                                            position = 'identity',color='black') + scale_fill_manual(values = c("blue", "red")) + theme_classic() + labs(x = "Process temperature (K)" , y = "Number of incidences", fill = 'HDF status')

pl3 <- ggplot(data = df_train,
              aes(x = air_temperature_k,
                  fill = factor(hdf))) + geom_histogram(alpha = 0.8,
                                                        position = 'identity',color='black') + scale_fill_manual(values = c("blue", "red")) + theme_classic() + labs(x = "Air temperature (K)" , y = "Number of incidences", fill = 'HDF status')

plot_grid(pl3,pl2, labels = c("Air temperatures","Process temperatures"))

```

As we can observe from \@ref(fig:temps), majority of the heat dissipation failures have occurred __at relatively higher values of air temperatures__. These air temperatures are observed to be __around 302.5 K__. Higher air temperatures invariably leads to lower value of heat dissipation which may cause heat dissipation failure and subsequently, machine failure.

Heat dissipation values are governed by the following heat transfer equation.

$$ \boxed{\Delta H = mC_p(T_{process} - T_{air})}$$
Based on the above equation, let us now study how the difference between process and air temperatures vary for heat dissipation failures.


```{r tempdiff, fig.cap="Heat dissipation failure based on temperature difference",fig.align='center'}
df_temp_diff <- df_train %>% select(c(process_temperature_k,air_temperature_k,hdf)) %>% mutate(temp_diff = process_temperature_k - air_temperature_k)


pl4 <- ggplot(data = df_temp_diff,
              aes(x = temp_diff,
                  fill = factor(hdf)),
              alpha = 0.6) + geom_histogram(alpha = 0.8,
                                            position = 'identity',color='black') + scale_fill_manual(values = c("blue", "red")) + theme_classic() + labs(x = "Temperature difference (K)" , y = "Number of incidences", fill = 'HDF status') + ggtitle("Heat dissipation failure based on temperature difference") + 
  
  annotate("segment",x = 5,
    y = 2500,xend = 7 ,
    yend = 5 ,arrow = arrow(type = "closed", 
                              length = unit(0.02, "npc"))
  ) +
  annotate("text",x = 5,
    y = 3800,colour = "red",
    label = 'High chances of HDF \n due to low temperature difference',
    size = unit(3, "pt")) + theme(axis.text.x = element_text(angle = 10,face = 'bold')) 
pl4
```

As illustrated by \@ref(fig:tempdiff) and based on the heat transfer equation, we can observe that __the majority of the heat dissipation failures occur at low temperature differences between process and air temperatures.__